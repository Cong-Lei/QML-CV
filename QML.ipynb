{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a332d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates import RandomLayers\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd18c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30   # Number of optimization epochs\n",
    "n_train = 60000    # Size of the train dataset\n",
    "n_test = 10000     # Size of the test dataset\n",
    "n_dim = 2         # 需要降到多少维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5c5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"QNN/data/\" # Data saving folder\n",
    "PREPROCESS = True          # If False, skip quantum processing and load data from SAVE_PATH\n",
    "PCA_DR = True               # 是否进行PCA降维处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af92f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist_dataset.load_data()\n",
    "\n",
    "# Reduce dataset size\n",
    "train_images = train_images[:n_train]\n",
    "train_labels = train_labels[:n_train]\n",
    "test_images = test_images[:n_test]\n",
    "test_labels = test_labels[:n_test]\n",
    "\n",
    "# Normalize pixel values within 0 and 1\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1fecf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum pre-processing of train images:\n",
      "60000/60000        \n",
      "Quantum pre-processing of test images:\n",
      "10000/10000        \r"
     ]
    }
   ],
   "source": [
    "def pca_with_origin_data(ori_data, n_dim):\n",
    "    #进行PCA降维\n",
    "    n_sample, a, b = np.shape(ori_data)\n",
    "    ori_data = ori_data.reshape(n_sample,a*b)\n",
    "    pca = decomposition.PCA(n_components = n_dim)\n",
    "    return pca.fit_transform(ori_data)\n",
    "\n",
    "if PCA_DR == True: \n",
    "    test_images = pca_with_origin_data(test_images,n_dim)\n",
    "    train_images = pca_with_origin_data(train_images,n_dim)\n",
    "    \n",
    "n_wires = int(np.ceil(np.log(n_dim) / np.log(2)))\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=n_wires)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "\n",
    "def circuit(f=None):\n",
    "    qml.AmplitudeEmbedding(features=f, wires=range(n_wires), normalize=True, pad_with=0.)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "def quanv(image):\n",
    "    img = image.flatten()\n",
    "    q_results = circuit(img)     \n",
    "    return q_results\n",
    "\n",
    "\n",
    "if PREPROCESS == True:\n",
    "    q_train_images = []\n",
    "    print(\"Quantum pre-processing of train images:\")\n",
    "    for idx, img in enumerate(train_images):\n",
    "        print(\"{}/{}        \".format(idx + 1, n_train), end=\"\\r\")\n",
    "        q_train_images.append(quanv(img))\n",
    "      \n",
    "    q_train_images = np.asarray(q_train_images)\n",
    "\n",
    "    q_test_images = []\n",
    "    print(\"\\nQuantum pre-processing of test images:\")\n",
    "    for idx, img in enumerate(test_images):\n",
    "        print(\"{}/{}        \".format(idx + 1, n_test), end=\"\\r\")\n",
    "        q_test_images.append(quanv(img))\n",
    "    q_test_images = np.asarray(q_test_images)\n",
    "\n",
    "    # Save pre-processed images\n",
    "    np.save(SAVE_PATH + \"q_train_images_\" + str(n_dim) + \".npy\", q_train_images)\n",
    "    np.save(SAVE_PATH + \"q_test_images_\" + str(n_dim) + \".npy\", q_test_images)\n",
    "\n",
    "\n",
    "# Load pre-processed images\n",
    "q_train_images = np.load(SAVE_PATH + \"q_train_images_\" + str(n_dim) + \".npy\")\n",
    "q_test_images = np.load(SAVE_PATH + \"q_test_images_\" + str(n_dim) + \".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34918d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15000/15000 - 8s - loss: 2.1462 - accuracy: 0.1964 - val_loss: 2.1253 - val_accuracy: 0.2092 - 8s/epoch - 554us/step\n",
      "Epoch 2/30\n",
      "15000/15000 - 8s - loss: 2.1219 - accuracy: 0.2088 - val_loss: 2.1231 - val_accuracy: 0.2119 - 8s/epoch - 524us/step\n",
      "Epoch 3/30\n",
      "15000/15000 - 8s - loss: 2.1209 - accuracy: 0.2056 - val_loss: 2.1222 - val_accuracy: 0.2138 - 8s/epoch - 531us/step\n",
      "Epoch 4/30\n",
      "15000/15000 - 8s - loss: 2.1206 - accuracy: 0.2076 - val_loss: 2.1218 - val_accuracy: 0.2178 - 8s/epoch - 529us/step\n",
      "Epoch 5/30\n",
      "15000/15000 - 8s - loss: 2.1206 - accuracy: 0.2097 - val_loss: 2.1223 - val_accuracy: 0.2037 - 8s/epoch - 532us/step\n",
      "Epoch 6/30\n",
      "15000/15000 - 8s - loss: 2.1205 - accuracy: 0.2087 - val_loss: 2.1222 - val_accuracy: 0.2157 - 8s/epoch - 530us/step\n",
      "Epoch 7/30\n",
      "15000/15000 - 8s - loss: 2.1206 - accuracy: 0.2070 - val_loss: 2.1216 - val_accuracy: 0.2144 - 8s/epoch - 529us/step\n",
      "Epoch 8/30\n",
      "15000/15000 - 8s - loss: 2.1206 - accuracy: 0.2072 - val_loss: 2.1219 - val_accuracy: 0.2203 - 8s/epoch - 533us/step\n",
      "Epoch 9/30\n",
      "15000/15000 - 8s - loss: 2.1206 - accuracy: 0.2094 - val_loss: 2.1227 - val_accuracy: 0.2053 - 8s/epoch - 537us/step\n",
      "Epoch 10/30\n",
      "15000/15000 - 8s - loss: 2.1206 - accuracy: 0.2075 - val_loss: 2.1216 - val_accuracy: 0.2126 - 8s/epoch - 529us/step\n",
      "Epoch 11/30\n",
      "15000/15000 - 8s - loss: 2.1205 - accuracy: 0.2096 - val_loss: 2.1222 - val_accuracy: 0.2098 - 8s/epoch - 528us/step\n",
      "Epoch 12/30\n",
      "15000/15000 - 8s - loss: 2.1205 - accuracy: 0.2096 - val_loss: 2.1215 - val_accuracy: 0.2110 - 8s/epoch - 529us/step\n",
      "Epoch 13/30\n",
      "15000/15000 - 8s - loss: 2.1206 - accuracy: 0.2086 - val_loss: 2.1217 - val_accuracy: 0.2122 - 8s/epoch - 530us/step\n",
      "Epoch 14/30\n",
      "15000/15000 - 8s - loss: 2.1204 - accuracy: 0.2078 - val_loss: 2.1218 - val_accuracy: 0.2158 - 8s/epoch - 519us/step\n",
      "Epoch 15/30\n",
      "15000/15000 - 8s - loss: 2.1205 - accuracy: 0.2072 - val_loss: 2.1218 - val_accuracy: 0.2165 - 8s/epoch - 530us/step\n",
      "Epoch 16/30\n",
      "15000/15000 - 8s - loss: 2.1205 - accuracy: 0.2101 - val_loss: 2.1218 - val_accuracy: 0.2132 - 8s/epoch - 532us/step\n",
      "Epoch 17/30\n",
      "15000/15000 - 8s - loss: 2.1204 - accuracy: 0.2079 - val_loss: 2.1212 - val_accuracy: 0.2113 - 8s/epoch - 533us/step\n",
      "Epoch 18/30\n",
      "15000/15000 - 8s - loss: 2.1206 - accuracy: 0.2087 - val_loss: 2.1221 - val_accuracy: 0.2159 - 8s/epoch - 536us/step\n",
      "Epoch 19/30\n",
      "15000/15000 - 8s - loss: 2.1204 - accuracy: 0.2093 - val_loss: 2.1226 - val_accuracy: 0.2144 - 8s/epoch - 533us/step\n",
      "Epoch 20/30\n",
      "15000/15000 - 8s - loss: 2.1206 - accuracy: 0.2069 - val_loss: 2.1220 - val_accuracy: 0.2147 - 8s/epoch - 533us/step\n",
      "Epoch 21/30\n",
      "15000/15000 - 8s - loss: 2.1205 - accuracy: 0.2092 - val_loss: 2.1231 - val_accuracy: 0.1965 - 8s/epoch - 533us/step\n",
      "Epoch 22/30\n",
      "15000/15000 - 8s - loss: 2.1206 - accuracy: 0.2076 - val_loss: 2.1216 - val_accuracy: 0.2153 - 8s/epoch - 521us/step\n",
      "Epoch 23/30\n",
      "15000/15000 - 8s - loss: 2.1204 - accuracy: 0.2101 - val_loss: 2.1218 - val_accuracy: 0.2152 - 8s/epoch - 530us/step\n",
      "Epoch 24/30\n",
      "15000/15000 - 8s - loss: 2.1204 - accuracy: 0.2075 - val_loss: 2.1223 - val_accuracy: 0.2149 - 8s/epoch - 528us/step\n",
      "Epoch 25/30\n",
      "15000/15000 - 8s - loss: 2.1205 - accuracy: 0.2094 - val_loss: 2.1222 - val_accuracy: 0.1981 - 8s/epoch - 532us/step\n",
      "Epoch 26/30\n",
      "15000/15000 - 8s - loss: 2.1206 - accuracy: 0.2076 - val_loss: 2.1214 - val_accuracy: 0.2163 - 8s/epoch - 532us/step\n",
      "Epoch 27/30\n",
      "15000/15000 - 8s - loss: 2.1205 - accuracy: 0.2089 - val_loss: 2.1222 - val_accuracy: 0.2145 - 8s/epoch - 527us/step\n",
      "Epoch 28/30\n",
      "15000/15000 - 8s - loss: 2.1204 - accuracy: 0.2092 - val_loss: 2.1220 - val_accuracy: 0.2087 - 8s/epoch - 526us/step\n",
      "Epoch 29/30\n",
      "15000/15000 - 8s - loss: 2.1205 - accuracy: 0.2093 - val_loss: 2.1217 - val_accuracy: 0.2139 - 8s/epoch - 526us/step\n",
      "Epoch 30/30\n",
      "15000/15000 - 8s - loss: 2.1205 - accuracy: 0.2072 - val_loss: 2.1219 - val_accuracy: 0.2084 - 8s/epoch - 528us/step\n"
     ]
    }
   ],
   "source": [
    "def MyModel():\n",
    "    \"\"\"Initializes and returns a custom Keras model\n",
    "    which is ready to be trained.\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "q_model = MyModel()\n",
    "\n",
    "#换下网络结构试试\n",
    "q_history = q_model.fit(\n",
    "    q_train_images,\n",
    "    train_labels,\n",
    "    validation_data=(q_test_images, test_labels),\n",
    "    batch_size=4,\n",
    "    epochs=n_epochs,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ba854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2d2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd1a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
