{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f26a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from tensorflow import keras\n",
    "from pennylane.optimize import NesterovMomentumOptimizer \n",
    "import cv2\n",
    "from skimage.feature import local_binary_pattern\n",
    "import random\n",
    "import pymrmr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747150c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30   # Number of optimization epochs    \n",
    "n_train = 100    # Size of the train dataset\n",
    "n_test = 60     # Size of the test dataset\n",
    "n_dim = 100       # 需要降到多少维\n",
    "target_label_list = [0,1] #需要提取数据的标签列表\n",
    "run_numbers = 5           #重复运行多少次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c940768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"QNN/data/\" # Data saving folder\n",
    "PREPROCESS = True          # If False, skip processing and load data from SAVE_PATH\n",
    "select_samples_with_labels = True       # 是否挑选特定标签的数据\n",
    "FS_state = True              # 是否进行图像特征提取\n",
    "fs_type = 'mRMR'               #图像特征提取的方式，'LBP','HOG','random','mRMR'\n",
    "load_random_index = False    # 是否加载随机选择特征的索引\n",
    "load_mRMR_index = True      # 是否加载mRMR算法选择特征的索引\n",
    "random_selected_feature_num = 300 #随机选择像素的个数 \n",
    "mnist_dataset = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist_dataset.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769da43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#做2分类的方案----抽取0和1作为数据集\n",
    "def extract_data_with_label(origin_data, origin_label, target_label_list):\n",
    "    new_data = []\n",
    "    new_labels = []\n",
    "    for i in range(len(origin_label)):\n",
    "        for j in range(len(target_label_list)):\n",
    "            if(origin_label[i] == target_label_list[j]):\n",
    "                new_data.append(origin_data[i,:,:])\n",
    "                new_labels.append(origin_label[i])\n",
    "    new_data = np.array(new_data)\n",
    "    new_labels = np.array(new_labels)\n",
    "    return new_data, new_labels\n",
    "\n",
    "if select_samples_with_labels == True:\n",
    "    test_images, test_labels = extract_data_with_label(test_images, test_labels, target_label_list)\n",
    "    train_images, train_labels = extract_data_with_label(train_images, train_labels, target_label_list)  \n",
    "\n",
    "# Reduce dataset size\n",
    "train_images = train_images[:n_train]\n",
    "train_labels = train_labels[:n_train]\n",
    "test_images = test_images[:n_test]\n",
    "test_labels = test_labels[:n_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ebc4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(image):\n",
    "    image -= image.min()\n",
    "    image = image / (image.max() - image.min())\n",
    "    image *= 255\n",
    "    image = image.astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "def extract_HOG_features(origin_image):\n",
    "    #由于MNIST数据集本身就是灰度图，所以不需要再转灰度图\n",
    "    origin_image = normalization(origin_image)\n",
    "    cell_size = (6,6)\n",
    "    num_cells_per_block = (2,2)\n",
    "    block_size = (num_cells_per_block[0] * cell_size[0], num_cells_per_block[1] * cell_size[1])\n",
    "    x_cells = origin_image.shape[1] // cell_size[0]\n",
    "    y_cells = origin_image.shape[0] // cell_size[1]\n",
    "    h_stride = 1\n",
    "    v_stride = 1\n",
    "    block_stride = (cell_size[0] * h_stride, cell_size[1] * v_stride)\n",
    "    num_bins = 9\n",
    "    win_size = (x_cells * cell_size[0], y_cells * cell_size[1])\n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, num_bins)\n",
    "    hog_descriptor = hog.compute(origin_image)\n",
    "    return hog_descriptor\n",
    "\n",
    "def fs_with_HOG(ori_imgs):\n",
    "    new_images = []\n",
    "    for idx, img in enumerate(ori_imgs):\n",
    "        new_images.append(extract_HOG_features(img))\n",
    "    new_images = np.array(new_images)\n",
    "    return new_images\n",
    "\n",
    "pixel_num = np.shape(test_images)[1] * np.shape(test_images)[2]\n",
    "\n",
    "\n",
    "def get_random_selected_list(selected_feature_num): #生成随机列表，用于选择特定的像素\n",
    "    selected_index_list = []\n",
    "\n",
    "    if load_random_index == True:\n",
    "        selected_index_list = np.load(SAVE_PATH + \"selected_index_list_\" + str(random_selected_feature_num) + \"_01.npy\")\n",
    "        selected_index_list = selected_index_list.tolist()\n",
    "    else: \n",
    "        for i in range(selected_feature_num):\n",
    "            selected_index_list.append(random.randint(0,pixel_num-1))\n",
    "\n",
    "        selected_index_list = np.array(selected_index_list, requires_grad=False)\n",
    "        np.save(SAVE_PATH + \"selected_index_list_\" + str(random_selected_feature_num) + \"_01.npy\", selected_index_list)\n",
    "\n",
    "    return selected_index_list\n",
    "\n",
    "\n",
    "def fs_with_random(ori_imgs, selected_index_list): # 随机选择若干个像素\n",
    "    new_images = []\n",
    "    selected_index_list.sort()\n",
    "    ori_imgs = np.reshape(ori_imgs,(np.shape(ori_imgs)[0],-1))\n",
    "\n",
    "    for i in range(len(selected_index_list)):\n",
    "        new_images.append(ori_imgs[:,selected_index_list[i]])\n",
    "\n",
    "    new_images = np.array(new_images, requires_grad=False)\n",
    "    new_images = np.transpose(new_images)\n",
    "    return new_images \n",
    "\n",
    "def fs_with_LBP(ori_imgs, radius): # radius为LBP算法中范围半径的取值\n",
    "    n_points = 8 * radius\n",
    "    new_images = []\n",
    "    for idx, img in enumerate(ori_imgs):\n",
    "        new_images.append(local_binary_pattern(img, n_points, radius))\n",
    "    new_images = np.array(new_images, requires_grad=False)\n",
    "    return new_images\n",
    "\n",
    "def fs_with_mRMR(ori_imgs, labels, n_dim):\n",
    "    labels = np.reshape(labels, (np.shape(labels)[0], -1))\n",
    "    data = np.concatenate((labels,ori_imgs), axis=1)\n",
    "\n",
    "    row_index_list = []\n",
    "    row_name = []\n",
    "    for i in range(len(labels)):\n",
    "        row_name = 'Row_' + str(i+1)\n",
    "        row_index_list.append(row_name)\n",
    "\n",
    "    column_index_list = []\n",
    "    column_name = []\n",
    "\n",
    "    for i in range(np.shape(ori_imgs)[1] + 1):\n",
    "        column_name = 'Colum_' + str(i+1)\n",
    "        column_index_list.append(column_name)\n",
    "\n",
    "    data_df = pd.DataFrame(data, index=row_index_list, columns = column_index_list)\n",
    "\n",
    "    if load_mRMR_index == False:\n",
    "        featuer_column_name_list = pymrmr.mRMR(data_df, 'MID', n_dim)\n",
    "        np.save(SAVE_PATH + \"mRMR_selected_index_list_\" + str(n_dim) + \"_01.npy\", featuer_column_name_list)\n",
    "    else:\n",
    "        featuer_column_name_list = np.load(SAVE_PATH + \"mRMR_selected_index_list_\" + str(n_dim) + \"_01.npy\")\n",
    "    new_images = data_df[featuer_column_name_list]\n",
    "    return np.array(new_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb9ec9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index_list = get_random_selected_list(random_selected_feature_num)\n",
    "\n",
    "def feature_selection(images, labels, fs_type):\n",
    "    processed_images = []\n",
    "    if fs_type == 'LBP':\n",
    "        processed_images = fs_with_LBP(images, 1)\n",
    "    elif fs_type == 'HOG':\n",
    "        processed_images = fs_with_HOG(images)\n",
    "    elif fs_type == 'random':\n",
    "        processed_images = fs_with_random(images, random_index_list)\n",
    "    elif fs_type == 'mRMR':\n",
    "        processed_images = fs_with_mRMR(images, labels, n_dim)\n",
    "\n",
    "    return processed_images \n",
    "\n",
    "if FS_state == True:\n",
    "    if fs_type != 'mRMR':\n",
    "       test_images = feature_selection(test_images, test_labels, fs_type)\n",
    "       train_images = feature_selection(train_images, train_labels, fs_type)\n",
    "    else:\n",
    "        train_images = np.reshape(train_images,(np.shape(train_images)[0],-1))\n",
    "        test_images = np.reshape(test_images,(np.shape(test_images)[0],-1))\n",
    "\n",
    "        data = np.concatenate((train_images,test_images))\n",
    "        labels = np.concatenate((train_labels,test_labels))\n",
    "        processed_data = feature_selection(data, labels, fs_type)\n",
    "        train_images = processed_data[:n_train]\n",
    "        data_number = n_train + n_test\n",
    "        test_images = processed_data[n_train:data_number,:]\n",
    "    \n",
    "# Normalize pixel values within 0 and 1\n",
    "train_images = train_images / (np.max(train_images) - np.min(train_images))\n",
    "test_images = test_images / (np.max(test_images) - np.min(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e3b69c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wires = int(np.ceil(np.log(n_dim) / np.log(2)))\n",
    "dev = qml.device(\"default.qubit\", wires=n_wires)\n",
    "\n",
    "def layer(W):\n",
    "\n",
    "    for i in range(n_wires):\n",
    "        qml.Rot(W[i,0], W[i,1], W[i,2], wires=i)\n",
    "        if i == n_wires - 1:\n",
    "            qml.CNOT(wires = [i, 0])\n",
    "        else:\n",
    "            qml.CNOT(wires = [i, i+1])\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(weights, f=False):\n",
    "    #初态制备\n",
    "    qml.AmplitudeEmbedding(f, wires=range(n_wires), normalize=True, pad_with=0.)\n",
    "\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37e6c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias\n",
    "\n",
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / len(labels)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def cost(weights, bias, features, labels):\n",
    "    predictions = [variational_classifier(weights, bias, f) for f in features]\n",
    "    return square_loss(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f329d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing of train images:\n",
      "1/100        \r",
      "2/100        \r",
      "3/100        \r",
      "4/100        \r",
      "5/100        \r",
      "6/100        \r",
      "7/100        \r",
      "8/100        \r",
      "9/100        \r",
      "10/100        \r",
      "11/100        \r",
      "12/100        \r",
      "13/100        \r",
      "14/100        \r",
      "15/100        \r",
      "16/100        \r",
      "17/100        \r",
      "18/100        \r",
      "19/100        \r",
      "20/100        \r",
      "21/100        \r",
      "22/100        \r",
      "23/100        \r",
      "24/100        \r",
      "25/100        \r",
      "26/100        \r",
      "27/100        \r",
      "28/100        \r",
      "29/100        \r",
      "30/100        \r",
      "31/100        \r",
      "32/100        \r",
      "33/100        \r",
      "34/100        \r",
      "35/100        \r",
      "36/100        \r",
      "37/100        \r",
      "38/100        \r",
      "39/100        \r",
      "40/100        \r",
      "41/100        \r",
      "42/100        \r",
      "43/100        \r",
      "44/100        \r",
      "45/100        \r",
      "46/100        \r",
      "47/100        \r",
      "48/100        \r",
      "49/100        \r",
      "50/100        \r",
      "51/100        \r",
      "52/100        \r",
      "53/100        \r",
      "54/100        \r",
      "55/100        \r",
      "56/100        \r",
      "57/100        \r",
      "58/100        \r",
      "59/100        \r",
      "60/100        \r",
      "61/100        \r",
      "62/100        \r",
      "63/100        \r",
      "64/100        \r",
      "65/100        \r",
      "66/100        \r",
      "67/100        \r",
      "68/100        \r",
      "69/100        \r",
      "70/100        \r",
      "71/100        \r",
      "72/100        \r",
      "73/100        \r",
      "74/100        \r",
      "75/100        \r",
      "76/100        \r",
      "77/100        \r",
      "78/100        \r",
      "79/100        \r",
      "80/100        \r",
      "81/100        \r",
      "82/100        \r",
      "83/100        \r",
      "84/100        \r",
      "85/100        \r",
      "86/100        \r",
      "87/100        \r",
      "88/100        \r",
      "89/100        \r",
      "90/100        \r",
      "91/100        \r",
      "92/100        \r",
      "93/100        \r",
      "94/100        \r",
      "95/100        \r",
      "96/100        \r",
      "97/100        \r",
      "98/100        \r",
      "99/100        \r",
      "100/100        \r\n",
      "pre-processing of test images:\n",
      "1/60        \r",
      "2/60        \r",
      "3/60        \r",
      "4/60        \r",
      "5/60        \r",
      "6/60        \r",
      "7/60        \r",
      "8/60        \r",
      "9/60        \r",
      "10/60        \r",
      "11/60        \r",
      "12/60        \r",
      "13/60        \r",
      "14/60        \r",
      "15/60        \r",
      "16/60        \r",
      "17/60        \r",
      "18/60        \r",
      "19/60        \r",
      "20/60        \r",
      "21/60        \r",
      "22/60        \r",
      "23/60        \r",
      "24/60        \r",
      "25/60        \r",
      "26/60        \r",
      "27/60        \r",
      "28/60        \r",
      "29/60        \r",
      "30/60        \r",
      "31/60        \r",
      "32/60        \r",
      "33/60        \r",
      "34/60        \r",
      "35/60        \r",
      "36/60        \r",
      "37/60        \r",
      "38/60        \r",
      "39/60        \r",
      "40/60        \r",
      "41/60        \r",
      "42/60        \r",
      "43/60        \r",
      "44/60        \r",
      "45/60        \r",
      "46/60        \r",
      "47/60        \r",
      "48/60        \r",
      "49/60        \r",
      "50/60        \r",
      "51/60        \r",
      "52/60        \r",
      "53/60        \r",
      "54/60        \r",
      "55/60        \r",
      "56/60        \r",
      "57/60        \r",
      "58/60        \r",
      "59/60        \r",
      "60/60        \r"
     ]
    }
   ],
   "source": [
    "def process_img_to_features(image):\n",
    "    img = image.flatten()\n",
    "    return img\n",
    "\n",
    "if PREPROCESS == True: #将图像数据拉成1维\n",
    "    new_train_images = []\n",
    "    print(\"pre-processing of train images:\")\n",
    "    for idx, img in enumerate(train_images):\n",
    "        print(\"{}/{}        \".format(idx + 1, np.shape(train_images)[0]), end=\"\\r\")\n",
    "        new_train_images.append(process_img_to_features(img))\n",
    "    train_images = np.array(new_train_images, requires_grad=False)\n",
    "\n",
    "    new_test_images = []\n",
    "    print(\"\\npre-processing of test images:\")\n",
    "    for idx, img in enumerate(test_images):\n",
    "        print(\"{}/{}        \".format(idx + 1, np.shape(test_images)[0]), end=\"\\r\")\n",
    "        new_test_images.append(process_img_to_features(img))\n",
    "    test_images = np.array(new_test_images, requires_grad=False)\n",
    "\n",
    "    # Save pre-processed images\n",
    "    np.save(SAVE_PATH + \"new_train_images_\" + str(n_dim) + fs_type + \"_01.npy\", train_images)\n",
    "    np.save(SAVE_PATH + \"new_test_images_\" + str(n_dim) + fs_type + \"_01.npy\", test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae13214c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\autograd\\numpy\\numpy_wrapper.py:156: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return A.astype(dtype, order, casting, subok, copy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 1.1406096 | Acc train: 0.2100000 | Acc validation: 0.2500000 \n",
      "Iter:     2 | Cost: 1.1243714 | Acc train: 0.2100000 | Acc validation: 0.2666667 \n",
      "Iter:     3 | Cost: 1.1052600 | Acc train: 0.2200000 | Acc validation: 0.3333333 \n",
      "Iter:     4 | Cost: 1.0824841 | Acc train: 0.3100000 | Acc validation: 0.3500000 \n",
      "Iter:     5 | Cost: 1.0526326 | Acc train: 0.4300000 | Acc validation: 0.5166667 \n",
      "Iter:     6 | Cost: 1.0137202 | Acc train: 0.6400000 | Acc validation: 0.7833333 \n",
      "Iter:     7 | Cost: 0.9721257 | Acc train: 0.7500000 | Acc validation: 0.8666667 \n",
      "Iter:     8 | Cost: 0.9269096 | Acc train: 0.7500000 | Acc validation: 0.8666667 \n",
      "Iter:     9 | Cost: 0.8846493 | Acc train: 0.7600000 | Acc validation: 0.8833333 \n",
      "Iter:    10 | Cost: 0.8438244 | Acc train: 0.7600000 | Acc validation: 0.8833333 \n",
      "Iter:    11 | Cost: 0.8054138 | Acc train: 0.7600000 | Acc validation: 0.8833333 \n",
      "Iter:    12 | Cost: 0.7672865 | Acc train: 0.7900000 | Acc validation: 0.8833333 \n",
      "Iter:    13 | Cost: 0.7314248 | Acc train: 0.8100000 | Acc validation: 0.8833333 \n",
      "Iter:    14 | Cost: 0.7000093 | Acc train: 0.8000000 | Acc validation: 0.8833333 \n",
      "Iter:    15 | Cost: 0.6727581 | Acc train: 0.8100000 | Acc validation: 0.8833333 \n",
      "Iter:    16 | Cost: 0.6497159 | Acc train: 0.8100000 | Acc validation: 0.8833333 \n",
      "Iter:    17 | Cost: 0.6273695 | Acc train: 0.8300000 | Acc validation: 0.8833333 \n",
      "Iter:    18 | Cost: 0.6071471 | Acc train: 0.8500000 | Acc validation: 0.8833333 \n",
      "Iter:    19 | Cost: 0.5859465 | Acc train: 0.8700000 | Acc validation: 0.8833333 \n",
      "Iter:    20 | Cost: 0.5672982 | Acc train: 0.8700000 | Acc validation: 0.8833333 \n",
      "Iter:    21 | Cost: 0.5497389 | Acc train: 0.8900000 | Acc validation: 0.9000000 \n",
      "Iter:    22 | Cost: 0.5313404 | Acc train: 0.9200000 | Acc validation: 0.9000000 \n",
      "Iter:    23 | Cost: 0.5156802 | Acc train: 0.9200000 | Acc validation: 0.9000000 \n",
      "Iter:    24 | Cost: 0.5007707 | Acc train: 0.9200000 | Acc validation: 0.9000000 \n",
      "Iter:    25 | Cost: 0.4866920 | Acc train: 0.9200000 | Acc validation: 0.9000000 \n",
      "Iter:    26 | Cost: 0.4726997 | Acc train: 0.9200000 | Acc validation: 0.9000000 \n",
      "Iter:    27 | Cost: 0.4611405 | Acc train: 0.9200000 | Acc validation: 0.9000000 \n",
      "Iter:    28 | Cost: 0.4518523 | Acc train: 0.9200000 | Acc validation: 0.9000000 \n",
      "Iter:    29 | Cost: 0.4442648 | Acc train: 0.9300000 | Acc validation: 0.9000000 \n",
      "Iter:    30 | Cost: 0.4355700 | Acc train: 0.9400000 | Acc validation: 0.9166667 \n",
      "best result----\n",
      "Iter:    30 | Cost: 0.4355700 | Acc train: 0.9400000 | Acc validation: 0.9166667 \n",
      "the number of features: 100\n",
      "Iter:     1 | Cost: 1.1163725 | Acc train: 0.2000000 | Acc validation: 0.3166667 \n",
      "Iter:     2 | Cost: 1.0939675 | Acc train: 0.3100000 | Acc validation: 0.4166667 \n",
      "Iter:     3 | Cost: 1.0745786 | Acc train: 0.4100000 | Acc validation: 0.5500000 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m weights, bias, _, _ \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mstep(cost, weights, bias, X_batch, Y_batch)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Compute predictions on train and validation set\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m predictions_train \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39msign(variational_classifier(weights, bias, f)) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m train_images]\n\u001b[0;32m     35\u001b[0m predictions_val \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39msign(variational_classifier(weights, bias, f)) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m test_images]\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Compute accuracy on train and validation set\u001b[39;00m\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     31\u001b[0m weights, bias, _, _ \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mstep(cost, weights, bias, X_batch, Y_batch)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Compute predictions on train and validation set\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m predictions_train \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39msign(\u001b[43mvariational_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m train_images]\n\u001b[0;32m     35\u001b[0m predictions_val \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39msign(variational_classifier(weights, bias, f)) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m test_images]\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Compute accuracy on train and validation set\u001b[39;00m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mvariational_classifier\u001b[1;34m(weights, bias, x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvariational_classifier\u001b[39m(weights, bias, x):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcircuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m bias\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pennylane\\qnode.py:619\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    612\u001b[0m using_custom_cache \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    613\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__setitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__delitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    616\u001b[0m )\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape_cached \u001b[38;5;241m=\u001b[39m using_custom_cache \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtape\u001b[38;5;241m.\u001b[39mhash \u001b[38;5;129;01min\u001b[39;00m cache\n\u001b[1;32m--> 619\u001b[0m res \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m    620\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtape],\n\u001b[0;32m    621\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m    622\u001b[0m     gradient_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_fn,\n\u001b[0;32m    623\u001b[0m     interface\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface,\n\u001b[0;32m    624\u001b[0m     gradient_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_kwargs,\n\u001b[0;32m    625\u001b[0m     override_shots\u001b[38;5;241m=\u001b[39moverride_shots,\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_kwargs,\n\u001b[0;32m    627\u001b[0m )\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m autograd\u001b[38;5;241m.\u001b[39misinstance(res, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# If a device batch transform was applied, we need to 'unpack'\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# the returned tuple/list to a float.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;66;03m# TODO: find a more explicit way of determining that a batch transform\u001b[39;00m\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;66;03m# was applied.\u001b[39;00m\n\u001b[0;32m    641\u001b[0m     res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pennylane\\interfaces\\execution.py:344\u001b[0m, in \u001b[0;36mexecute\u001b[1;34m(tapes, device, gradient_fn, interface, mode, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform)\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch_fn(res)\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gradient_fn \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m interface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch_fn(\n\u001b[1;32m--> 344\u001b[0m         \u001b[43mqml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterfaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_execute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_fn\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m     )\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# the default execution function is batch_execute\u001b[39;00m\n\u001b[0;32m    350\u001b[0m execute_fn \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39minterfaces\u001b[38;5;241m.\u001b[39mcache_execute(batch_execute, cache, expand_fn\u001b[38;5;241m=\u001b[39mexpand_fn)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pennylane\\interfaces\\execution.py:172\u001b[0m, in \u001b[0;36mcache_execute.<locals>.wrapper\u001b[1;34m(tapes, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (res, []) \u001b[38;5;28;01mif\u001b[39;00m return_tuple \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# execute all unique tapes that do not exist in the cache\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m     res \u001b[38;5;241m=\u001b[39m fn(execution_tapes\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m final_res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tape \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tapes):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pennylane\\interfaces\\execution.py:97\u001b[0m, in \u001b[0;36mcache_execute.<locals>.fn\u001b[1;34m(tapes, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(tapes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# pylint: disable=function-redefined\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     tapes \u001b[38;5;241m=\u001b[39m [expand_fn(tape) \u001b[38;5;28;01mfor\u001b[39;00m tape \u001b[38;5;129;01min\u001b[39;00m tapes]\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_fn(tapes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pennylane\\_qubit_device.py:355\u001b[0m, in \u001b[0;36mQubitDevice.batch_execute\u001b[1;34m(self, circuits)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m circuit \u001b[38;5;129;01min\u001b[39;00m circuits:\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;66;03m# we need to reset the device here, else it will\u001b[39;00m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;66;03m# not start the next computation in the zero state\u001b[39;00m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m--> 355\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(res)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracker\u001b[38;5;241m.\u001b[39mactive:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pennylane\\_qubit_device.py:257\u001b[0m, in \u001b[0;36mQubitDevice.execute\u001b[1;34m(self, circuit, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_validity(circuit\u001b[38;5;241m.\u001b[39moperations, circuit\u001b[38;5;241m.\u001b[39mobservables)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# apply all circuit operations\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(circuit\u001b[38;5;241m.\u001b[39moperations, rotations\u001b[38;5;241m=\u001b[39mcircuit\u001b[38;5;241m.\u001b[39mdiagonalizing_gates, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# generate computational basis samples\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshots \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m circuit\u001b[38;5;241m.\u001b[39mis_sampled:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pennylane\\devices\\default_qubit.py:231\u001b[0m, in \u001b[0;36mDefaultQubit.apply\u001b[1;34m(self, operations, rotations, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debugger\u001b[38;5;241m.\u001b[39msnapshots[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debugger\u001b[38;5;241m.\u001b[39msnapshots)] \u001b[38;5;241m=\u001b[39m state_vector\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_operation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# store the pre-rotated state\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_rotated_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pennylane\\devices\\default_qubit.py:256\u001b[0m, in \u001b[0;36mDefaultQubit._apply_operation\u001b[1;34m(self, state, operation)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m operation\u001b[38;5;241m.\u001b[39mbase_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_ops:\n\u001b[0;32m    255\u001b[0m     axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwires\u001b[38;5;241m.\u001b[39mindices(wires)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_ops\u001b[49m\u001b[43m[\u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_unitary_matrix(operation), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC_DTYPE)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m operation \u001b[38;5;129;01min\u001b[39;00m diagonal_in_z_basis:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pennylane\\devices\\default_qubit.py:376\u001b[0m, in \u001b[0;36mDefaultQubit._apply_cnot\u001b[1;34m(self, state, axes, **kwargs)\u001b[0m\n\u001b[0;32m    373\u001b[0m     target_axes \u001b[38;5;241m=\u001b[39m [axes[\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m    375\u001b[0m state_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_x(state[sl_1], axes\u001b[38;5;241m=\u001b[39mtarget_axes)\n\u001b[1;32m--> 376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43msl_0\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_x\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pennylane\\numpy\\wrapper.py:117\u001b[0m, in \u001b[0;36mtensor_wrapper.<locals>._wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m         tensor_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _np\u001b[38;5;241m.\u001b[39many([i\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tensor_args])\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# evaluate the original object\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m res \u001b[38;5;241m=\u001b[39m obj(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, _np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# only if the output of the object is a ndarray,\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# then convert to a PennyLane tensor\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     res \u001b[38;5;241m=\u001b[39m tensor(res, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtensor_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\autograd\\numpy\\numpy_wrapper.py:88\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(arrays, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# this code is basically copied from numpy/core/shape_base.py's stack\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# we need it here because we want to re-implement stack in terms of the\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# primitives defined in this file\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [array(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\autograd\\numpy\\numpy_wrapper.py:88\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(arrays, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# this code is basically copied from numpy/core/shape_base.py's stack\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# we need it here because we want to re-implement stack in terms of the\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# primitives defined in this file\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\autograd\\numpy\\numpy_wrapper.py:60\u001b[0m, in \u001b[0;36marray\u001b[1;34m(A, *args, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array_from_args(args, kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(array, A))\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_from_scalar_or_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\autograd\\tracer.py:48\u001b[0m, in \u001b[0;36mprimitive.<locals>.f_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_box(ans, trace, node)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\autograd\\numpy\\numpy_wrapper.py:73\u001b[0m, in \u001b[0;36m_array_from_scalar_or_array\u001b[1;34m(array_args, array_kwargs, scalar)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;129m@primitive\u001b[39m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_from_scalar_or_array\u001b[39m(array_args, array_kwargs, scalar):\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _np\u001b[38;5;241m.\u001b[39marray(scalar, \u001b[38;5;241m*\u001b[39marray_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marray_kwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_images = np.load(SAVE_PATH + \"new_train_images_\" + str(n_dim) + fs_type + \"_01.npy\")\n",
    "test_images = np.load(SAVE_PATH + \"new_test_images_\" + str(n_dim) + fs_type + \"_01.npy\")\n",
    "\n",
    "train_labels = train_labels * 2 - np.ones(len(train_labels)) # shift label from {0, 1} to {-1, 1}\n",
    "test_labels  =  test_labels * 2 - np.ones(len(test_labels))\n",
    "\n",
    "\n",
    "result_all = []   \n",
    "for i in range(run_numbers):\n",
    "    #初始化参数\n",
    "    num_qubits = n_wires\n",
    "    num_layers = 6     #设置为2,4,6\n",
    "    weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "    bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "    opt = NesterovMomentumOptimizer(0.01)\n",
    "    batch_size = 5\n",
    "\n",
    "    weights = weights_init\n",
    "    bias = bias_init\n",
    "\n",
    "    best_result_list = [0,0,0,0]  #保存每次实验的最佳结果\n",
    "\n",
    "    for it in range(n_epochs):\n",
    "\n",
    "        # Update the weights by one optimizer step\n",
    "        batch_index = np.random.randint(0, len(train_images), (batch_size,))\n",
    "        X_batch = train_images[batch_index]\n",
    "        Y_batch = train_labels[batch_index]\n",
    "        \n",
    "        weights, bias, _, _ = opt.step(cost, weights, bias, X_batch, Y_batch)\n",
    "        \n",
    "        # Compute predictions on train and validation set\n",
    "        predictions_train = [np.sign(variational_classifier(weights, bias, f)) for f in train_images]\n",
    "        predictions_val = [np.sign(variational_classifier(weights, bias, f)) for f in test_images]\n",
    "        \n",
    "        # Compute accuracy on train and validation set\n",
    "        acc_train = accuracy(train_labels, predictions_train)\n",
    "        acc_val = accuracy(test_labels, predictions_val)\n",
    "\n",
    "        print(\n",
    "            \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n",
    "            \"\".format(it + 1, cost(weights, bias, train_images, train_labels), acc_train, acc_val)\n",
    "        )\n",
    "        if(acc_val > best_result_list[3]):\n",
    "            best_result_list = [it + 1, cost(weights, bias, train_images, train_labels), acc_train, acc_val]\n",
    "        elif(acc_val == best_result_list[3]):\n",
    "            if(cost(weights, bias, train_images, train_labels) < best_result_list[1]):\n",
    "                best_result_list = [it + 1, cost(weights, bias, train_images, train_labels), acc_train, acc_val]\n",
    "\n",
    "    print(\n",
    "        \"best result----\\nIter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n",
    "        \"\".format(best_result_list[0], best_result_list[1], best_result_list[2], best_result_list[3])\n",
    "    )\n",
    "\n",
    "    print('the number of features:', np.shape(test_images)[1])\n",
    "\n",
    "    result_all.append(best_result_list)\n",
    "\n",
    "def get_average_test_accuracy_and_variance(result_all):\n",
    "    acc_list = []\n",
    "    for i in range(len(result_all)):\n",
    "        acc_list.append(result_all[i][-1])\n",
    "\n",
    "    ave_test_acc = np.mean(acc_list)\n",
    "    test_acc_var = np.var(acc_list)\n",
    "    return ave_test_acc, test_acc_var\n",
    "\n",
    "for i in range(len(result_all)):\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n",
    "        \"\".format(result_all[i][0], result_all[i][1], result_all[i][2], result_all[i][3])\n",
    "    )\n",
    "\n",
    "ave_test_acc, test_acc_var = get_average_test_accuracy_and_variance(result_all)\n",
    "\n",
    "print( \"average test accuracy: {:0.7f} | test accuracy variance: {:0.7f} \"\n",
    "    \"\".format(ave_test_acc, test_acc_var)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
